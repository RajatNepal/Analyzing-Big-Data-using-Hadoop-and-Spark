{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing in Spark"
      ],
      "metadata": {
        "id": "8QEhrl6tiT2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up\n",
        "\n",
        "(copied from lab 5)"
      ],
      "metadata": {
        "id": "lo3bUhcTiZwu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZb00ewyiP97",
        "outputId": "23a5500a-fd61-4a50-8cfc-08b855909ffc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#getting google drive mounted\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "!wget https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.2-bin-hadoop3.tgz\n",
        "!rm spark-3.3.2-bin-hadoop3.tgz \n",
        "\n",
        "# Setting up our environmental variables: \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n",
        "\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyPsD3Aqie5q",
        "outputId": "959ac56d-a8aa-4fe3-ce2b-1a08314017e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-09 18:19:27--  https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 299360284 (285M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.3.2-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.3.2-bin-had 100%[===================>] 285.49M   176MB/s    in 1.6s    \n",
            "\n",
            "2023-03-09 18:19:28 (176 MB/s) - ‘spark-3.3.2-bin-hadoop3.tgz’ saved [299360284/299360284]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) #  This will format our output tables a bit nicer when not using the show() method\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "th4Ni345ie8V",
        "outputId": "38223691-083c-41af-8468-e31f42dc107b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f8a5c0c3700>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://5f344d2981f2:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copying files over and removing whitespace"
      ],
      "metadata": {
        "id": "OoiyRKfYlQG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bringing in stopwords file from drive into current directory\n",
        "!cp /content/drive/MyDrive/project/stopwords.txt /content/\n",
        "!mkdir articles\n",
        "#copying all article files from drive to current directory\n",
        "!cp /content/drive/MyDrive/project/wikipedia-ml/article* /content/articles/\n",
        "\n",
        "#removing white space to put all files in hadoop\n",
        "#hadoop didnt like it when there were spaces in the file name\n",
        "#inspiration for this: https://stackoverflow.com/questions/2709458/how-to-replace-spaces-in-file-names-using-a-bash-script\n",
        "!for f in articles/*\\ *; do mv \"$f\" \"${f// /_}\"; done"
      ],
      "metadata": {
        "id": "qUKwT4yWie-3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MapReduce-like operations\n",
        "\n",
        "So the way I did it, I have things stored in text files since I am not too profecient with lambda functions yet. If I were in a real-world setting, I would use lambda functions to simplify the code and not rely on txt files."
      ],
      "metadata": {
        "id": "AtH_SV8a_9v_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Co Occurence"
      ],
      "metadata": {
        "id": "f1mYznB3Asrp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Co Occurence map like function"
      ],
      "metadata": {
        "id": "8TIsDetdAxdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext\n",
        "with open(\"stopwords.txt\") as s:\n",
        "    stopwords = set([line.strip() for line in s])\n",
        "\n",
        "path = \"/content/articles/\"\n",
        "\n",
        "#iterate through our years given\n",
        "for year in range(2013, 2024):\n",
        "  for filename in os.listdir(path):\n",
        "    #only continue if the file is in the year we want\n",
        "    if str(year) in filename:\n",
        "\n",
        "      #similar to map reduce, this is the map part\n",
        "      #opening the files we want\n",
        "      with open(path + filename) as f:\n",
        "\n",
        "        #reading lines and replacing the punctuation\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "          line = line.replace(\".\", \"\").replace(\",\", \"\").replace(\"?\",\"\").replace(\"!\",\"\")\n",
        "          words = line.strip().split()\n",
        "          for i in range(len(words)):\n",
        "            # If the word is a stopword, skip it\n",
        "            if words[i] in stopwords:\n",
        "                continue\n",
        "            # Iterate over the words in the window\n",
        "            for j in range(i + 1, len(words)):\n",
        "            # If the word is a stopword, skip it\n",
        "                if words[j] in stopwords:\n",
        "                    continue\n",
        "              # Output the co-occurrence pairs in a text file for the given year\n",
        "              #this giving the map output in a text file which the reduce will use as input\n",
        "              #to aggregate\n",
        "              #make sure we use \"a\" in the open method so it appends and not rewrites\n",
        "                with open(f\"output_{year}.txt\", \"a\") as o:\n",
        "                    print('%s\\t%s\\t%s' % (words[i], words[j], 1), file=o)\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "chNZvRvzifBR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Co Occurence reducer like function"
      ],
      "metadata": {
        "id": "6cucDQ_7A3ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#now that we have all the output txt files, we can read them in\n",
        "#and perform a reduce for each year\n",
        "delimiter = '\\t'\n",
        "\n",
        "for year in range(2013, 2024):\n",
        "  #i like using python dictionaries to store stuff, so here it is\n",
        "  myDict = {}\n",
        "  #read through lines of each year's map output\n",
        "  with open(f\"/content/output_{year}.txt\") as f:\n",
        "    lines = f.readlines();\n",
        "    for line in lines:\n",
        "      first, second, count = line.strip().split(delimiter)\n",
        "      key = (first + \",\" + second)\n",
        "      count = int(count)\n",
        "\n",
        "      #simple frequency table\n",
        "      if key in myDict.keys():\n",
        "        myDict[key] += count\n",
        "      else:\n",
        "        myDict[key] = 1\n",
        "\n",
        "#putting this all in a new output file for each year\n",
        "#sperating values with commas instead of tabs and funky stuff like in hadoop\n",
        "#also adding the headers to make the spark schema easier\n",
        "#this will make loading into pyspark much easier\n",
        "  with open(f\"reduced_{year}.txt\", \"a\") as r:\n",
        "    print(\"word_0,word_1,count\",file=r)\n",
        "    for key in myDict.keys():\n",
        "      print(key + \",\" + str(myDict[key]),file=r)\n"
      ],
      "metadata": {
        "id": "OUOQOnr_vzkY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Co occurence: putting it into spark dataframes and calculating stuff"
      ],
      "metadata": {
        "id": "bkhHzWPFA7Ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#now that the reducer has done its job, we can sift through the output\n",
        "#and put all of it in a dataframe\n",
        "dfs={}\n",
        "\n",
        "for year in range(2013, 2024):\n",
        "  dfs[year] = spark.read.options(delimiter=',', header=True, inferSchema=True).csv(f\"/content/reduced_{year}.txt\")\n"
      ],
      "metadata": {
        "id": "k41EsrekvznS"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for year in range(2013, 2024):\n",
        "  print(year)\n",
        "  #dfs[year].printSchema()\n",
        "  dfs[year].orderBy(\"count\", ascending=False).show(10)\n",
        "\n",
        "#dfs[2014].select(\"*\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfkU3_X2vzsf",
        "outputId": "a2580269-47b2-4fc0-9732-f0c0ea258eea"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2013\n",
            "+----------+--------------+-----+\n",
            "|    word_0|        word_1|count|\n",
            "+----------+--------------+-----+\n",
            "|  learning|      learning|   73|\n",
            "|  learning|    algorithms|   60|\n",
            "|  learning|          data|   58|\n",
            "|   machine|      learning|   52|\n",
            "|  learning|representation|   44|\n",
            "|algorithms|representation|   44|\n",
            "|algorithms|      learning|   39|\n",
            "|algorithms|      features|   39|\n",
            "|algorithms|    algorithms|   38|\n",
            "|  learning|           can|   36|\n",
            "+----------+--------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "2014\n",
            "+----------+--------------+-----+\n",
            "|    word_0|        word_1|count|\n",
            "+----------+--------------+-----+\n",
            "|  learning|      learning|   71|\n",
            "|  learning|          data|   58|\n",
            "|  learning|    algorithms|   55|\n",
            "|   machine|      learning|   54|\n",
            "|  learning|           can|   35|\n",
            "|algorithms|      learning|   35|\n",
            "|algorithms|representation|   34|\n",
            "|  learning|representation|   34|\n",
            "|algorithms|      features|   33|\n",
            "|algorithms|    algorithms|   32|\n",
            "+----------+--------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "2015\n",
            "+----------+----------+-----+\n",
            "|    word_0|    word_1|count|\n",
            "+----------+----------+-----+\n",
            "|  learning|  learning|  121|\n",
            "|   machine|  learning|   69|\n",
            "|  learning|algorithms|   64|\n",
            "|  learning|      data|   64|\n",
            "|   Machine|  learning|   42|\n",
            "|  learning|      time|   36|\n",
            "|  learning|  training|   34|\n",
            "|algorithms|  learning|   32|\n",
            "|  learning|       can|   29|\n",
            "|  learning|   methods|   29|\n",
            "+----------+----------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "2016\n",
            "+----------+----------+-----+\n",
            "|    word_0|    word_1|count|\n",
            "+----------+----------+-----+\n",
            "|  learning|  learning|  124|\n",
            "|   machine|  learning|   75|\n",
            "|  learning|algorithms|   72|\n",
            "|  learning|      data|   72|\n",
            "|   Machine|  learning|   46|\n",
            "|  learning|  training|   42|\n",
            "|algorithms|  learning|   37|\n",
            "|  learning|      time|   36|\n",
            "|  learning|       can|   33|\n",
            "|  learning|       set|   32|\n",
            "+----------+----------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "2017\n",
            "+--------+----------+-----+\n",
            "|  word_0|    word_1|count|\n",
            "+--------+----------+-----+\n",
            "|learning|  learning|  185|\n",
            "| machine|  learning|  137|\n",
            "|learning|      data|  109|\n",
            "|learning|algorithms|   76|\n",
            "|learning|   machine|   59|\n",
            "|learning|  training|   56|\n",
            "|    data|      data|   56|\n",
            "| Machine|  learning|   54|\n",
            "|    data|  learning|   53|\n",
            "|learning| knowledge|   52|\n",
            "+--------+----------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "2018\n",
            "+----------+----------+-----+\n",
            "|    word_0|    word_1|count|\n",
            "+----------+----------+-----+\n",
            "|  learning|  learning|  188|\n",
            "|   machine|  learning|  154|\n",
            "|  learning|      data|  123|\n",
            "|  learning|algorithms|   73|\n",
            "|  learning|   machine|   69|\n",
            "|      data|      data|   68|\n",
            "|  learning| knowledge|   66|\n",
            "|   machine|      data|   60|\n",
            "|      data|  learning|   58|\n",
            "|algorithms|  learning|   55|\n",
            "+----------+----------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "2019\n",
            "+----------+----------+-----+\n",
            "|    word_0|    word_1|count|\n",
            "+----------+----------+-----+\n",
            "|  learning|  learning|  446|\n",
            "|   machine|  learning|  259|\n",
            "|  learning|      data|  257|\n",
            "|      data|      data|  217|\n",
            "|  learning|algorithms|  205|\n",
            "|algorithms|  learning|  205|\n",
            "|      data|  learning|  181|\n",
            "|  learning|   machine|  176|\n",
            "|  learning|  training|  136|\n",
            "|   Machine|  learning|  115|\n",
            "+----------+----------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "2020\n",
            "+----------+----------+-----+\n",
            "|    word_0|    word_1|count|\n",
            "+----------+----------+-----+\n",
            "|  learning|  learning|  426|\n",
            "|   machine|  learning|  307|\n",
            "|  learning|      data|  249|\n",
            "|      data|      data|  205|\n",
            "|  learning|   machine|  193|\n",
            "|      data|  learning|  183|\n",
            "|  learning|algorithms|  179|\n",
            "|  learning|  training|  154|\n",
            "|algorithms|  learning|  139|\n",
            "|      data|  training|  119|\n",
            "+----------+----------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "2021\n",
            "+--------+----------+-----+\n",
            "|  word_0|    word_1|count|\n",
            "+--------+----------+-----+\n",
            "|learning|  learning|  423|\n",
            "| machine|  learning|  358|\n",
            "|learning|      data|  249|\n",
            "|    data|      data|  214|\n",
            "|learning|   machine|  204|\n",
            "|    data|  learning|  184|\n",
            "|learning|algorithms|  169|\n",
            "|learning|  training|  158|\n",
            "|    data|  training|  130|\n",
            "| machine|   machine|  121|\n",
            "+--------+----------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "2022\n",
            "+--------+----------+-----+\n",
            "|  word_0|    word_1|count|\n",
            "+--------+----------+-----+\n",
            "|learning|  learning|  474|\n",
            "| machine|  learning|  409|\n",
            "|learning|      data|  285|\n",
            "|    data|      data|  245|\n",
            "|learning|   machine|  243|\n",
            "|    data|  learning|  220|\n",
            "|learning|  training|  163|\n",
            "|learning|algorithms|  162|\n",
            "| machine|   machine|  149|\n",
            "| machine|      data|  139|\n",
            "+--------+----------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "2023\n",
            "+--------+----------+-----+\n",
            "|  word_0|    word_1|count|\n",
            "+--------+----------+-----+\n",
            "|learning|  learning|  234|\n",
            "| machine|  learning|  210|\n",
            "|learning|      data|  145|\n",
            "|    data|      data|  125|\n",
            "|learning|   machine|  118|\n",
            "|    data|  learning|  114|\n",
            "|learning|  training|   81|\n",
            "|learning|algorithms|   76|\n",
            "| machine|      data|   76|\n",
            "| machine|   machine|   73|\n",
            "+--------+----------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#deleting these files now because the data is already stored in\n",
        "#our dataframe\n",
        "!rm output*\n",
        "!rm reduced*"
      ],
      "metadata": {
        "id": "k-8sDXMT7fkO"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trigrams"
      ],
      "metadata": {
        "id": "2L8eGjvSBFYp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trigrams map like function"
      ],
      "metadata": {
        "id": "-DSGico3BH5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#now doing similar map-reduce-spark for trigrams\n",
        "#map part\n",
        "\n",
        "#copying some of the initial code to read files from before\n",
        "\n",
        "path = \"/content/articles/\"\n",
        "\n",
        "#iterate through our years given\n",
        "for year in range(2013, 2024):\n",
        "  for filename in os.listdir(path):\n",
        "    #only continue if the file is in the year we want\n",
        "    if str(year) in filename:\n",
        "\n",
        "      #similar to map reduce, this is the map part\n",
        "      #opening the files we want\n",
        "      with open(path + filename) as f:\n",
        "\n",
        "        #reading lines and replacing the punctuation\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "          line = line.replace(\".\", \"\").replace(\",\", \"\").replace(\"?\",\"\").replace(\"!\",\"\")\n",
        "          words = line.strip().split()\n",
        "          for i in range(len(words) -2):\n",
        "            key = (words[i] + \",\" + words[i+1] + \",\"+words[i+2])\n",
        "\n",
        "            with open(f\"output2_{year}.txt\", \"a\") as o:\n",
        "              print('%s\\t%s' % (key, 1),file=o)\n",
        "                    \n",
        "            "
      ],
      "metadata": {
        "id": "RvVhiXhi7fnv"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trigrams reduce like function"
      ],
      "metadata": {
        "id": "h1_2VAtJBKWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#reduce part of trigram\n",
        "#most of this code is copied from the previous co occurence reducer\n",
        "delimiter = '\\t'\n",
        "\n",
        "for year in range(2013, 2024):\n",
        "  #i like using python dictionaries to store stuff, so here it is\n",
        "  myDict = {}\n",
        "  #read through lines of each year's map output\n",
        "  with open(f\"/content/output2_{year}.txt\") as f:\n",
        "    lines = f.readlines();\n",
        "    for line in lines:\n",
        "      key, count = line.strip().split(delimiter)\n",
        "      \n",
        "      count = int(count)\n",
        "\n",
        "      #simple frequency table\n",
        "      if key in myDict.keys():\n",
        "        myDict[key] += count\n",
        "      else:\n",
        "        myDict[key] = 1\n",
        "\n",
        "#putting this all in a new output file for each year\n",
        "#sperating values with commas instead of tabs and funky stuff like in hadoop\n",
        "#also adding the headers to make the spark schema easier\n",
        "#this will make loading into pyspark much easier\n",
        "  with open(f\"reduced2_{year}.txt\", \"a\") as r:\n",
        "    print(\"word_0,word_1,word_2,count\",file=r)\n",
        "    for key in myDict.keys():\n",
        "      print(key + \",\" + str(myDict[key]),file=r)\n",
        "\n"
      ],
      "metadata": {
        "id": "ggu8VVFp7frF"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trigram putting into spark dataframes and doing operations on it"
      ],
      "metadata": {
        "id": "HjIA45ZYBNYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dfs2={}\n",
        "\n",
        "for year in range(2013, 2024):\n",
        "  dfs2[year] = spark.read.options(delimiter=',', header=True, inferSchema=True).csv(f\"/content/reduced2_{year}.txt\")\n",
        "\n"
      ],
      "metadata": {
        "id": "XBnv8RDe_FQW"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for year in range(2013, 2024):\n",
        "  print(year)\n",
        "  dfs2[year].orderBy(\"count\", ascending=False).show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn1lY4_O_klt",
        "outputId": "97de23e4-1025-4720-eb65-b60bbf7c6af8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2013\n",
            "+-------------+----------+--------+-----+\n",
            "|       word_0|    word_1|  word_2|count|\n",
            "+-------------+----------+--------+-----+\n",
            "|            a|       set|      of|   11|\n",
            "|           of|   machine|learning|   10|\n",
            "|           in|polynomial|    time|    6|\n",
            "|         with|   respect|      to|    6|\n",
            "|   algorithms|   attempt|      to|    6|\n",
            "|           be|      used|      to|    6|\n",
            "|computational|  learning|  theory|    6|\n",
            "|      results|      show|    that|    4|\n",
            "|         spam|       and|non-spam|    4|\n",
            "|           be|   learned|      in|    4|\n",
            "+-------------+----------+--------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "2014\n",
            "+-------------+----------+----------+-----+\n",
            "|       word_0|    word_1|    word_2|count|\n",
            "+-------------+----------+----------+-----+\n",
            "|           of|   machine|  learning|   12|\n",
            "|            a|       set|        of|   10|\n",
            "|           in|polynomial|      time|    6|\n",
            "|           be|      used|        to|    6|\n",
            "|   algorithms|   attempt|        to|    6|\n",
            "|computational|  learning|    theory|    6|\n",
            "|         with|   respect|        to|    6|\n",
            "|           be|   learned|        in|    4|\n",
            "|          For|   example|         a|    4|\n",
            "|      learned|        in|polynomial|    4|\n",
            "+-------------+----------+----------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "2015\n",
            "+-------------+----------+----------+-----+\n",
            "|       word_0|    word_1|    word_2|count|\n",
            "+-------------+----------+----------+-----+\n",
            "|           of|   machine|  learning|   12|\n",
            "|            a|       set|        of|   10|\n",
            "|      Machine|  learning|        is|    7|\n",
            "|         with|   respect|        to|    6|\n",
            "|   algorithms|   attempt|        to|    6|\n",
            "|           in|polynomial|      time|    6|\n",
            "|      machine|  learning|algorithms|    6|\n",
            "|         been|   applied|        in|    6|\n",
            "|      Machine|  learning|       and|    6|\n",
            "|computational|  learning|    theory|    5|\n",
            "+-------------+----------+----------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "2016\n",
            "+-------------+----------+----------+-----+\n",
            "|       word_0|    word_1|    word_2|count|\n",
            "+-------------+----------+----------+-----+\n",
            "|           of|   machine|  learning|   13|\n",
            "|            a|       set|        of|   10|\n",
            "|      Machine|  learning|        is|    8|\n",
            "|computational|  learning|    theory|    6|\n",
            "|            a|  training|       set|    6|\n",
            "|           in|polynomial|      time|    6|\n",
            "|         with|   respect|        to|    6|\n",
            "|      machine|  learning|algorithms|    6|\n",
            "|   algorithms|   attempt|        to|    6|\n",
            "|         been|   applied|        in|    6|\n",
            "+-------------+----------+----------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "2017\n",
            "+----------+--------+----------+-----+\n",
            "|    word_0|  word_1|    word_2|count|\n",
            "+----------+--------+----------+-----+\n",
            "|         a|     set|        of|   15|\n",
            "|        of| machine|  learning|   13|\n",
            "|   Machine|learning|        is|    8|\n",
            "|   machine|learning|        is|    8|\n",
            "|   machine|learning|algorithms|    8|\n",
            "|  learning|      is|         a|    7|\n",
            "|        be|    used|        to|    6|\n",
            "|complexity|      of|       the|    6|\n",
            "|        is|       a|    method|    6|\n",
            "|        In|addition|        to|    6|\n",
            "+----------+--------+----------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "2018\n",
            "+-------------+-----------+----------+-----+\n",
            "|       word_0|     word_1|    word_2|count|\n",
            "+-------------+-----------+----------+-----+\n",
            "|            a|        set|        of|   16|\n",
            "|           of|    machine|  learning|   12|\n",
            "|      machine|   learning|        is|   11|\n",
            "|      Machine|   learning|        is|    8|\n",
            "|      machine|   learning|algorithms|    8|\n",
            "|     learning|         is|         a|    8|\n",
            "|          the|      field|        of|    7|\n",
            "|computational|   learning|    theory|    6|\n",
            "|   algorithms|    attempt|        to|    6|\n",
            "|          the|performance|        of|    6|\n",
            "+-------------+-----------+----------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "2019\n",
            "+----------+--------+----------+-----+\n",
            "|    word_0|  word_1|    word_2|count|\n",
            "+----------+--------+----------+-----+\n",
            "|         a|     set|        of|   30|\n",
            "|        of| machine|  learning|   20|\n",
            "|   machine|learning|algorithms|   14|\n",
            "|        in|     the|      data|   12|\n",
            "|       the|training|      data|   12|\n",
            "|       can|      be|      used|   12|\n",
            "|        of|       a|       set|   12|\n",
            "|algorithms|     are|      used|   12|\n",
            "|   machine|learning|        is|   11|\n",
            "|       set|      of|      data|   11|\n",
            "+----------+--------+----------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "2020\n",
            "+----------+--------+----------+-----+\n",
            "|    word_0|  word_1|    word_2|count|\n",
            "+----------+--------+----------+-----+\n",
            "|         a|     set|        of|   26|\n",
            "|        of| machine|  learning|   22|\n",
            "|   machine|learning|algorithms|   13|\n",
            "|       the|   field|        of|   11|\n",
            "|       the|training|      data|   11|\n",
            "|       can|      be|      used|   11|\n",
            "|algorithms|     are|      used|   10|\n",
            "|   machine|learning|        is|   10|\n",
            "|        in|     the|      data|   10|\n",
            "|        be|    used|        to|   10|\n",
            "+----------+--------+----------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "2021\n",
            "+-------+--------+----------+-----+\n",
            "| word_0|  word_1|    word_2|count|\n",
            "+-------+--------+----------+-----+\n",
            "|     of| machine|  learning|   27|\n",
            "|      a|     set|        of|   23|\n",
            "|machine|learning|        is|   14|\n",
            "|machine|learning|algorithms|   13|\n",
            "|    the|   field|        of|   12|\n",
            "|      a| machine|  learning|   12|\n",
            "|    the|training|      data|   10|\n",
            "|     In|addition|        to|   10|\n",
            "|     of|       a|       set|   10|\n",
            "|    can|      be|      used|   10|\n",
            "+-------+--------+----------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "2022\n",
            "+-------+--------+----------+-----+\n",
            "| word_0|  word_1|    word_2|count|\n",
            "+-------+--------+----------+-----+\n",
            "|     of| machine|  learning|   32|\n",
            "|      a|     set|        of|   24|\n",
            "|machine|learning|        is|   14|\n",
            "|    the|   field|        of|   12|\n",
            "|machine|learning|algorithms|   12|\n",
            "|      a| machine|  learning|   11|\n",
            "|    the|training|      data|   10|\n",
            "|     In|addition|        to|   10|\n",
            "|     of|       a|       set|   10|\n",
            "|    can|      be|      used|   10|\n",
            "+-------+--------+----------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "2023\n",
            "+-------+--------+----------+-----+\n",
            "| word_0|  word_1|    word_2|count|\n",
            "+-------+--------+----------+-----+\n",
            "|     of| machine|  learning|   17|\n",
            "|      a|     set|        of|   13|\n",
            "|machine|learning|        is|    7|\n",
            "|    the|   field|        of|    6|\n",
            "|machine|learning|algorithms|    6|\n",
            "|    the|training|      data|    5|\n",
            "|     In|addition|        to|    5|\n",
            "|     of|       a|       set|    5|\n",
            "|    can|      be|      used|    5|\n",
            "|      a| machine|  learning|    5|\n",
            "+-------+--------+----------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#deleting these files now because the data is already stored in\n",
        "#our dataframe\n",
        "!rm output*\n",
        "!rm reduced*"
      ],
      "metadata": {
        "id": "GpLRcBJk_u6Z"
      },
      "execution_count": 45,
      "outputs": []
    }
  ]
}